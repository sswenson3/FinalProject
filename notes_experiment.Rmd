---
title: "Developing a Metric for Measuring Translation Faithfulness R Notebook"
output: html_notebook
---
## Introduction

Basic Text analytics, also known as text mining, can be used to glean information or data from unstructured sources such as books or prose.  Initially, after watching a video on YouTube (Moffit 2020) talking about Introduction of color into literature has a specific order I wondered whether we could use text analytics to determine approximately when in a civilization a work was created.  Expanding on that perhaps we could identify a specific period that a work was created based on it's thumbprint derived from analyzing its text as data. One of the works cited in the video was "The Iliad" and that the word blue was not used.  So I performed a quick analysis of this work.

Immediately, a problem was encountered. Performing the initial analysis showed that translators had not been faithful to the original text!  

```{r,eval=F, echo = T}
Illiad %>% colorWords()
#### A tibble: 6 x 2
  word       n
  <chr>  <int>
1 black     99
2 blue      14
3 green     12
4 red       16
5 white     50
6 yellow    10


```

This lead to the question of whether I can judge the reliability of a translation by using text analytics on a set of commonly used adjectives.   I chose adjectives as they are part of descriptions and less likely to fall prone to idiomatic use with meanings other than literal like verbs and nouns might. 

I would construct a word frequency analysis and propose a rule of measure.  The words used would be common adjectives such as colors, sizes, distances. We would then attempt to develop a distance metric to show how 'close' a given translation is to the original text. The original text we would use will be the Aeneid by Virgil, a Latin saga about the mythic origins of the Roman people following the fall of Troy, sort of a sequel/spinoff of Homer's Odyssey saga.   


## Packages Required

- tidyverse -- Provides access to dplyer and tidy tools for handling data frames
- tidytext  -- Provides tidy style tools for text mining both analytics and analysis.
- hunspell  -- This library provides access to some NLP (Natural Language Processing) functions that will allow us to perform deeper analysis based on language rules and relations.  This library is one that is popularly used to perform spellchecking in many systems and applications.
- glue      -- This provides a simple variable interpolation interface, simplifying some syntax in coding. 
- gutenbergr -- This library provides access to the The Gutenburg Project metadata and full text of more than 60000 works in various languages.
- ggplot2   -- This allows us to perform various graphing tasks 


### Development Libraries 

- ggiraphExtra -- This allows us to perform various graphing tasks such as interactive plots 
- ggiraph      -- To create different flavors of radar graphs
- ggradar      -- To create different flavors of radar graphs
    
    

Loading our Libraries 
```{r}
library(tidyverse)
library(tidytext)
library(hunspell)
library(glue)
library(gutenbergr)
library(ggplot2)
#devtools::install_github("ricardo-bion/ggradar", dependencies = TRUE)
#devtools::install_github("cardiomoon/ggiraphExtra")
library(ggiraphExtra)
library(ggiraph)
library(ggradar)
```

## Data Generation and Preparation 

First we must prepare the system itself for the languages we will be using Latin and English. To do this I will be installing a library  reference for hunspell to use and store that in my Data Folder in this project.
Incidentally these libraries are the same as required to perform spell checking in a particular natural language and can be used system wide.


Latin Library 
The English library is already part of the standard library loaded into the system.  

  






Utility functions 
```{r}

#let me send in lists of words as space separated strings for convenience
qq <- function(alist) {  list = strsplit(alist," ", fixed= T); list  = list[[1]] }

# Basic word frequency analysis on a provided text
colorwords2 = function(df) {
    wordhist <- df %>% unnest_tokens(word,text) %>% 
        filter( word %in% c("white","black","red","yellow","green","blue","orange","indigo", "violet" ) ) %>% count(word)
    wordhist }

```



```{r}
# Load up the Latin Hunspell 
latin = dictionary("Data/la_LA.dic")
#TESTING 
# w = tibble ( verba = qq(
#   "Atra atrae Atrae atram atra ater atri atro atrum atro atrater atorum atris atrior atrissima atrissimum atrissimis" ) )
w = tibble ( verba = qq(
  "bonus bonum boni bonas bonis bonissimus bona bonae bonas" ) )
df <- w %>% mutate(
  stem=  hunspell_stem(verba, dict=latin )
) %>% unnest(stem)
w

df
```


Here we run into a problem there is ambiguity in the returned results.   So... we will ignore this for now and construct a concordance of stems associated with the adjectives we wish to use in our analysis. In this example the duplicates are all generated by alternate spellings of the same word.  We will use a dictionary to help us remove unrelated stems where they occur. 


### Generating  and cleaning concordanance Data 

Retrieving data from a declension generator for each of our selected words
filtering out unwanted text from that process 
creating the concordance for that word
adding to the concordance table.

Each word will create a declension file derived from the generator website 
```{r}
exampledeclension <- read_file("Data/ater.declension.raw")
exampledeclension
```
This raw file is then converted to string containing all forms of the word and then converted to a list of words. 
```{r}
declension <- exampledeclension 
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
#get rid of unwanted characters from html table
declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
declension <- str_replace_all(declension, "\\s+"," ")  
declension <- str_replace_all(declension, "^\\s|\\s$","")
declension 

print (qq(declension))
```

 Finally, the forms are stemmed by the hunspell library and converted to a mini-concordance 

```{r}
english = "black"
verba = w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin)) %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))

verba
```
 
 In this case for the word ater, I know that neither atrium nor atrivm are valid forms for this word
 so we remove those rows and then add them to our concordance 
 
```{r}
verba <- verba %>% filter(!LatinStem %in% qq("atrium atrivm"))

concordance <- verba
concordance
```
 
I introduce a function to do this cleanup work for each word.  It is still necessary to inspect the mini concordance before adding its rows to our concordance

```{r}

# Regular expression excluding all unwanted text from a declension table
# generated at https://latin.cactus2000.de/noun/shownoun_en.php?n=generator
# "^m|\sf\s|n$|,|\.|^.\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|^Adverb.*$|Superlative|Comparative",""
string_declension <- function (verbum) {
    declension <- read_file(glue ("./Data/{verbum}.declension.raw"))
    #get rid of line control characters 
	  declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
    declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
    declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,|/|-"," ")
    #get rid of unwanted characters from html table
    declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
	
    declension <- str_replace_all(declension, "\\s+"," ")

    declension <- str_replace_all(declension, "\\s+"," ")
	declension <- str_replace_all(declension, "^\\s|\\s$","")
}

get_declension <- function (verbum, english, forms=NULL) {
    if (is.null(forms)){
      declension <- string_declension(glue({verbum}))
    } else {
      declension <- forms
    }
    w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin))      %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))
}

```

 The selected words for our concordance are : 
 
 black, white, red, yellow, green, blue, purple, orange
 
 And a  collection of common Latin adjectives:
 kind, right (direction), left , bad, good, diligent, many, only,
 none, one, swift, strong,old  
```{r}

```
 
The hypothesis is that cross section of common descriptive words could give us a sense 
of faithfulness to an original edition or translation.  This approach can later be expanded by including synonyms in each language of interest and some context via NLP could assist in discriminating a bit further. 


 # Examine w , remove invalid stems , mutate to add the english word to concordance

```{r}
niger <- get_declension("niger","black")
niger


```

  This word has no ambiguous forms
  
 

```{r}
concordance <- add_row(concordance,niger)

albus <- get_declension("albus","white")
albus


```

This has two  ambiguous or invalid forms for the word I want: albeo, alboris 

```{r}
albus <- albus %>% filter(!LatinStem %in% qq("albeo alboris"))
concordance <- add_row(concordance, albus)

concordance

```
```{r}
candidus <- get_declension("candidus","white")
candidus
```

 

```{r}

concordance <- add_row(concordance, candidus)

concordance

```




```{r}
ruber <- get_declension("ruber","red")
ruber
```
This has a single ambiguous stem 

```{r}
ruber <- ruber %>% filter(!LatinStem %in% qq("rubrus"))
concordance <- add_row(concordance, ruber)
concordance
```
```{r}
```


```{r}
flavus <- get_declension("flavus","yellow")
flavus
```
```{r}
flavus <- flavus %>% filter(!LatinStem %in% qq("flo flaveo"))
concordance <- add_row(concordance, flavus)
concordance

```

```{r}
fulvus <- get_declension("fulvus","yellow")
fulvus
```
```{r}
concordance <- add_row(concordance, fulvus)
concordance

```


```{r}
viridis <- get_declension("viridis","green")
viridis

```

```{r}
viridis <- viridis %>% filter(!LatinStem %in% qq("virido"))
concordance <- add_row(concordance, viridis)
concordance
```

```{r}
caeruleus <- get_declension("caeruleus","blue")
caeruleus

```

This is interesting.  This word has a compound comparative and superlative but preserves the earlier forms.  

```{r}
declension <- string_declension("caeruleus") 
declension
```
So, we'll just omit the helping adverb for blue for our concordance since the  unadorned stem will still appear anywhere it is used. 

```{r}
caeruleus <- caeruleus %>% filter( ! grepl("m.*",LatinStem))
caeruleus
concordance <- add_row(concordance, caeruleus)
concordance
```
The next word does the same thing so I only loaded the Positive declension into its file 
```{r}
croceus <- get_declension("croceus","orange")
croceus
```
```{r}
concordance <- add_row(concordance, croceus)
concordance

```
```{r}
purpureus <- get_declension("purpureus","purple")
purpureus
```
```{r}
concordance <- add_row(concordance, purpureus)
concordance
```

```{r}
amicus <- get_declension("amicus","kind")

```
```{r}
amicus <- amicus %>% filter(!LatinStem %in% qq("amicvi amicio"))
concordance<- add_row(concordance,amicus)
concordance
```

The word Dexter's declension is different.   It has alternate forms available in it's declension.  So I've refactored the declension utilities. Added the '/' as a delimiter to handle.

```{r}

declension <- read_file("Data/dexter.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```
```{r}
declension <- string_declension("dexter")
declension
```
That worked so proceeding as before. 
```{r}
dexter <- get_declension("dexter","right")
dexter
```
All of these are valid so adding to the concordance. 

```{r}
concordance<- add_row(concordance,dexter)
concordance
```
```{r}
sinister <- get_declension("sinister","left")
sinister
```
```{r}
concordance<- add_row(concordance,sinister)
concordance
```
Malus, or 'evil,bad' turns out to be irregular and changes its form. I find this amusing that evil doesn't follow the rules.  
```{r}
print(string_declension("malus"))
```
The declension doesn't appear to cause any problems with tools developed so far so we'll proceed as usual 

```{r}
malus <- get_declension("malus","bad")
malus
```
```{r}
malus <- malus %>% filter(! LatinStem %in% qq("peioro malvi ") )
concordance <- add_row (concordance, malus)
concordance

bonus <- get_declension("bonus","good")
bonus
```

```{r}
bonus <- bonus %>% filter(! LatinStem %in% qq("melioro melium") )
concordance <- add_row (concordance, bonus)
concordance

```
```{r}
diligens <- get_declension("diligens","diligent")
diligens
```

```{r}
diligens <- diligens %>% filter (! LatinStem %in% qq("diligo"))
concordance <- add_row(concordance,diligens)
concordance
```
We run into another issue in the irregulat declension of multus where certain forms don't exist and are represented by  - 

```{r}
declension <- read_file("Data/multus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```
Added a rule to our regular expression 

```{r}
print(string_declension("multus"))
multus <- get_declension("multus","many")
multus 
```
And proceeded as usual 
```{r}
multus <- multus %>% filter (! LatinStem %in% qq("pluvi"))
concordance <- add_row(concordance,multus)
concordance
```

```{r}
solus <- get_declension("solus","only")
solus
```

```{r}
solus <- solus %>% filter (! LatinStem %in% qq("solvi soleo"))
concordance <- add_row(concordance,solus)
concordance
```

```{r}

declension <- read_file("Data/nullus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension
```

Another twist. this time it will take manual intervention to clean it.
First we will collect all the words. remove the extraneous endings then add the missing forms and then stem the result into a mini-concordance, and verify for valid forms.  We will modify get_declension to accept verba as a list.

Regular expressions were not matching characters as expected, using other methods to remove "ī" and 
"ō" from the list.  Got position by matching REGEX wildcard for single character entries  and then used lapply/sapply and indexing to remove the entries. 


```{r}
declension <- qq(string_declension("nullus"))
declension
# remove  single character entries 
remove <- lapply (declension, function(ch) grep ("^.$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 )]
declension
```

Next remove ae and then add the alternate entries
```{r}
remove <- lapply (declension, function(ch) grep ("^ae$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 ) ]
declension <- c(declension, qq("nūllī nūllae nūllī nūllō nūllae nūllō"))
declension


```
```{r}
nullus <- get_declension("nullus","none",forms = declension)
nullus
concordance<-add_row(concordance,nullus)
concordance
```
```{r}
unus <- get_declension("unus","one")
unus
unus <- unus %>% filter(!LatinStem %in% qq("unii unio"))
concordance <- add_row(concordance,unus)
concordance
```
```{r}
celer <- get_declension("celer","swift")
celer

celer <- celer %>% filter(!LatinStem %in% qq("celo celero"))
concordance <- add_row(concordance,celer)
concordance
```


```{r}
fortis <- get_declension("fortis","strong")
fortis
concordance <- add_row(concordance,fortis)
concordance
```
```{r}
vetus <- get_declension("vetus","old")
vetus
```
```{r}
vetus <- vetus %>% filter(!LatinStem %in% qq("i ii vetero vetvi vetero veto veterrimus veterrimvs vetustus"))
vetus
concordance<-add_row(concordance,vetus)
concordance


```
Adding a lookup function: 
```{r}
lookup = function(word) { 
  concordance %>% 
    filter(LatinStem == word) %>% 
    select(English)
}

```



This completes our proposed concordance.  Next we'll load in an original Latin manuscript, tokenize it , stem these words , Join our concordance to this then perform an frequency analysis on the result.   This will be our base line to compare translations. 

Using gutenbergr I will load the text of the Aenied in Latin and several of its english translations 
.

```{r}
gutenberg_metadata %>% select(title,author,gutenberg_id,language) %>% filter(grepl("[Ae|Æ]neid",title) & author=="Virgil")

TheLatinAeneid <- gutenberg_download(227)   
TheLatinAeneid
TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid228
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid18466
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid22456
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid29358
TranslationAeneid49844   <- gutenberg_download(49844)
TranslationAeneid49844


```
```{r}

TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid49844   <- gutenberg_download(49844)


```
Now that we have a source with concordance to look at and five different translations we can start to ask some questions.  First how do they compare in raw size? 

```{r}
Text_sizes <- tibble (title = "TheLatinAeneid", wordcount =  TheLatinAeneid %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid228", wordcount =  TranslationAeneid228 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid18466", wordcount =  TranslationAeneid18466 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid22456", wordcount =  TranslationAeneid22456 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid29358", wordcount =  TranslationAeneid29358 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid49844", wordcount =  TranslationAeneid49844 %>% unnest_tokens(word,text)%>% nrow())

Text_sizes
Text_sizes %>% ggplot( aes(title,wordcount))+
  geom_col(fill = ifelse(Text_sizes$title =="TheLatinAeneid", 'red','black'))+coord_flip()+theme(axis.text.x = element_text(angle = 90))
```


```{r}
theme(axis.text.x = element_text(angle = 90))
```


```{r}
```


```{r}
```


Right away we can see that the information contained in each edition varies by a significant amount. This can be explained by front matter difference, different approaches to translation, and additional matter inserted into the edition.

So the next thing to attempt... can we see how faithful each edition is to the original? To do this we will attempt to construct a word frequency 'yardstick'   from the original Latin work and apply it to each of the editions in turn.  

```{r}
Analysis <-TheLatinAeneid %>% unnest_tokens(word,text)
Analysis$LatinStem<- Analysis$word %>% hunspell_stem( dict=latin ) %>% lapply( function(x) paste(x, collapse = " ")) %>% unlist()
```

```{r}

Analysis<-Analysis %>% mutate(English="")
#foreach row of analysis 
for( row in 1:nrow(Analysis)){
  stem <- Analysis[row, "LatinStem"]
  if ( ! is_tibble(stem)){ 
    print("not a tibble")
  }else{
    #print(glue("row:{row}:tibble:{stem}"))
    #glimpse(stem)
    stems = qq(stem %>% pull())
    if (length(stems) < 1) {
      #print ("skipping")
      next
    }
    for(LatinStem in stems) {
      english <- lookup(LatinStem) %>% pull()
      if ( length(english ) == 0 ){
        next
      }else{
        Analysis[row,"English"] = english
      }
    }#end for 
  }#Else-fi
}#For row 
    


```
Now that we have an English word assigned to each Latin word that is in our concordance we can perform a word-frequency analysis on the original Latin work.  We'll do this for each edition and display them separately.
First we will create a list of our concordance words to filter the WF analysis for each work we generate.
```{r}

wf = Analysis %>% select(English) %>% filter(! English =="" ) %>% count(English)
wf
```
Then we will build up our WF results row by row, each row representing a different edition.
```{r}
concordance_words = as.vector( concordance %>% select(English) %>% pull())
wfAnalysis<- wf %>% pivot_wider(names_from = English, values_from = n) %>% mutate(WorkTitle = 'TheLatinAeneid') %>% select(WorkTitle,everything())
wfAnalysis



```
```{r}
wf = TranslationAeneid228 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid228') %>% select(WorkTitle,everything())

wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid18466 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid18466') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid22456 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid22456') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid29358 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid29358') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid49844 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid49844') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)
#clean up for radar graph, turn NA to 0 
wfAnalysis_radar <-wfAnalysis
wfAnalysis_radar[is.na(wfAnalysis_radar)]<-0
wfAnalysis_radar
```
With our WF table complete we can graph the WF chart for each work 
```{r}

#Convert data to long form for ggplot tools 
longwfAnalysis = wfAnalysis_radar %>%  pivot_longer(!WorkTitle, names_to = "words", values_to= "count")
works = wfAnalysis_radar %>% select(WorkTitle) %>% pull()
works

# colors <- c('red','green','blue','purple','black','orange' )

 for ( i in 1:length(works)) {
   WFtitle = works[i]
   p <- ggplot(longwfAnalysis %>% filter(WorkTitle == WFtitle), aes(words,count,fill=WorkTitle)) + 
     geom_col()+
     scale_fill_manual(values = c(colors[i]))+
     theme(axis.text.x = element_text(angle = 90)) + 
     labs(title = paste(WFtitle,"- Concordance Word Frequency Analysis"))
 print(p)
 }

```
## Conclusion

It appears that a collection of common adjectives can be shown to give a measure of 'reliability'  of a translation.  

## Further Research 

A number of things could enhance this investigation, expanding to multiple ediotions of the same book in the same language should show us a 'control'  in natural divergence as more material is added or revised in each edition.   

Additional context clues using NLP could enhance or eliminate a need for a concordance.  A broader concordance might capture a more solid signature. Development of a metric to quantify 'distance'  from the original or first edition could be ddeveloped based on the wf-signature of the original document.   

```{r}


#wfAnalysis_radar
i=2
colors <- c('red','green','blue','purple','black','orange' )
for ( i in 2:6){
 # wfAnalysis_radar %>% slice(1,i)
  p <- ggradar( 
      wfAnalysis_radar %>% slice(1,i),
      values.radar = c("0", "140", "280"),
      grid.min = 0,
      grid.mid = 140,
      grid.max = 280,
      group.line.width = 2,
      group.point.size = 1,
      group.colours = qq(paste(colors[1], colors[i] ))


      ) + labs( title = "labs")
  #
  print(p)
}

```

```{r}
   ggRadar(data=wfAnalysis_radar %>% slice(1,2),aes(color= WorkTitle),interactive=T,alpha = .5, rescale = F )
   ggRadar(data=wfAnalysis_radar %>% slice(1,3),aes(color=WorkTitle),interactive=T,alpha = .5, rescale = F)
   ggRadar(data=wfAnalysis_radar %>% slice(1,4),aes(color=WorkTitle),interactive=T,alpha = .5, rescale = F )
   ggRadar(data=wfAnalysis_radar %>% slice(1,5),aes(color=WorkTitle),interactive=T,alpha = .5, rescale = F )
   ggRadar(data=wfAnalysis_radar %>% slice(1,6),aes(color=WorkTitle),interactive=T,alpha = .5, rescale = F )
  
   ggRadar(data=wfAnalysis_radar,aes(color= WorkTitle, facet=WorkTitle),interactive=T,alpha = .5, rescale = F, size = 1 )
   
#}
```


