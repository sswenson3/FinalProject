---
title: "R Notebook"
output: html_notebook
---

Loading our Libraries 
```{r}
library(DBI)
library(tidyverse)
library(tidytext)
library(hunspell)
library(glue)
library(gutenbergr)
library(ggplot2)
library(jsonlite)
library(httr)
```

Utility functions 
```{r}

#let me send in  lists of words as space separated strings for convenience
qq <- function(alist) {  list = strsplit(alist," ", fixed= T); list  = list[[1]] }


colorwords2 = function(df) {
    wordhist <- df %>% unnest_tokens(word,text) %>% 
        filter( word %in% c("white","black","red","yellow","green","blue","orange","indigo", "violet" ) ) %>% count(word)
    wordhist }




 
```



```{r}
# Load up the Latin Hunspell 
latin = dictionary("Data/la_LA.dic")
#TESTING 
# w = tibble ( verba = qq(
#   "Atra atrae Atrae atram atra ater atri atro atrum atro atrater atorum atris atrior atrissima atrissimum atrissimis" ) )
w = tibble ( verba = qq(
  "bonus bonum boni bonas bonis bonissimus bona bonae bonas" ) )
df <- w %>% mutate(
  stem=  hunspell_stem(verba, dict=latin )
) %>% unnest(stem)
w

df
```


Here we run into a problem there is ambiguity in the returned results.   So... we will ignore this for now and construct a concordance of stems associated with the adjectives we wish to use in our analysis. In this example the duplicates are all generated by alternate spellings of the same word.  We will use a dictionary to help us remove unrelated stems where they occur. 


### Generating  and cleaning concordanance Data 

Retrieving data from a declension generator for each of our selected words
filtering out unwanted text from that process 
creating the concordance for that word
adding to the concordance table.

Each word will create a declension file derived from the generator website 
```{r}
exampledeclension <- read_file("Data/ater.declension.raw")
exampledeclension
```
This raw file is then converted to string containing all forms of the word and then converted to a list of words. 
```{r}
declension <- exampledeclension 
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
#get rid of unwanted characters from html table
declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
declension <- str_replace_all(declension, "\\s+"," ")  
declension <- str_replace_all(declension, "^\\s|\\s$","")
declension 

print (qq(declension))
```

 Finally, the forms are stemmed by the hunspell library and converted to a mini-concordance 

```{r}
english = "black"
verba = w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin)) %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))

verba
```
 
 In this case for the word ater, I know that neither atrium nor atrivm are valid forms for this word
 so we remove those rows and then add them to our concordance 
 
```{r}
verba <- verba %>% filter(!LatinStem %in% qq("atrium atrivm"))

concordance <- verba
concordance
```
 
I introduce a function to do this cleanup work for each word.  It is still necessary to inspect the mini concordance before adding its rows to our concordance

```{r}

# Regular expression excluding all unwanted text from a declension table
# generated at https://latin.cactus2000.de/noun/shownoun_en.php?n=generator
# "^m|\sf\s|n$|,|\.|^.\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|^Adverb.*$|Superlative|Comparative",""
string_declension <- function (verbum) {
    declension <- read_file(glue ("./Data/{verbum}.declension.raw"))
    #get rid of line control characters 
	  declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
    declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
    declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,|/|-"," ")
    #get rid of unwanted characters from html table
    declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
	
    declension <- str_replace_all(declension, "\\s+"," ")

    declension <- str_replace_all(declension, "\\s+"," ")
	declension <- str_replace_all(declension, "^\\s|\\s$","")
}

get_declension <- function (verbum, english, forms=NULL) {
    if (is.null(forms)){
      declension <- string_declension(glue({verbum}))
    } else {
      declension <- forms
    }
    w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin))      %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))
}

```

 The selected words for our concordance are : 
 
 black, white, red, yellow, green, blue, purple, orange
 
 And a  collection of common Latin adjectives:
 kind, right (direction), left , bad, good, diligent, many, only,
 none, one, swift, strong,old  
```{r}

```
 
The hypothesis is that cross section of common descriptive words could give us a sense 
of faithfulness to an original edition or translation.  This approach can later be expanded by including synonyms in each language of interest and some context via NLP could assist in discriminating a bit further. 


 # Examine w , remove invalid stems , mutate to add the english word to concordance

```{r}
niger <- get_declension("niger","black")
niger


```

  This word has no ambiguous forms
  
 

```{r}
concordance <- add_row(concordance,niger)

albus <- get_declension("albus","white")
albus


```

This has two  ambiguous or invalid forms for the word I want: albeo, alboris 

```{r}
albus <- albus %>% filter(!LatinStem %in% qq("albeo alboris"))
concordance <- add_row(concordance, albus)

concordance

```
```{r}
candidus <- get_declension("candidus","white")
candidus
```

 

```{r}

concordance <- add_row(concordance, candidus)

concordance

```




```{r}
ruber <- get_declension("ruber","red")
ruber
```
This has a single ambiguous stem 

```{r}
ruber <- ruber %>% filter(!LatinStem %in% qq("rubrus"))
concordance <- add_row(concordance, ruber)
concordance
```
```{r}
```


```{r}
flavus <- get_declension("flavus","yellow")
flavus
```
```{r}
flavus <- flavus %>% filter(!LatinStem %in% qq("flo flaveo"))
concordance <- add_row(concordance, flavus)
concordance

```

```{r}
fulvus <- get_declension("fulvus","yellow")
fulvus
```
```{r}
concordance <- add_row(concordance, fulvus)
concordance

```


```{r}
viridis <- get_declension("viridis","green")
viridis

```

```{r}
viridis <- viridis %>% filter(!LatinStem %in% qq("virido"))
concordance <- add_row(concordance, viridis)
concordance
```

```{r}
caeruleus <- get_declension("caeruleus","blue")
caeruleus

```

This is interesting.  This word has a compound comparative and superlative but preserves the earlier forms.  

```{r}
declension <- string_declension("caeruleus") 
declension
```
So, we'll just omit the helping adverb for blue for our concordance since the  unadorned stem will still appear anywhere it is used. 

```{r}
caeruleus <- caeruleus %>% filter( ! grepl("m.*",LatinStem))
caeruleus
concordance <- add_row(concordance, caeruleus)
concordance
```
The next word does the same thing so I only loaded the Positive declension into its file 
```{r}
croceus <- get_declension("croceus","orange")
croceus
```
```{r}
concordance <- add_row(concordance, croceus)
concordance

```
```{r}
purpureus <- get_declension("purpureus","purple")
purpureus
```
```{r}
concordance <- add_row(concordance, purpureus)
concordance
```

```{r}
amicus <- get_declension("amicus","kind")

```
```{r}
amicus <- amicus %>% filter(!LatinStem %in% qq("amicvi amicio"))
concordance<- add_row(concordance,amicus)
concordance
```

The word Dexter's declension is different.   It has alternate forms available in it's declension.  So I've refactored the declension utilities. Added the '/' as a delimiter to handle.

```{r}

declension <- read_file("Data/dexter.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```
```{r}
declension <- string_declension("dexter")
declension
```
That worked so proceeding as before. 
```{r}
dexter <- get_declension("dexter","right")
dexter
```
All of these are valid so adding to the concordance. 

```{r}
concordance<- add_row(concordance,dexter)
concordance
```
```{r}
sinister <- get_declension("sinister","left")
sinister
```
```{r}
concordance<- add_row(concordance,sinister)
concordance
```
Malus, or 'evil,bad' turns out to be irregular and changes its form. I find this amusing that evil doesn't follow the rules.  
```{r}
print(string_declension("malus"))
```
The declension doesn't appear to cause any problems with tools developed so far so we'll proceed as usual 

```{r}
malus <- get_declension("malus","bad")
malus
```
```{r}
malus <- malus %>% filter(! LatinStem %in% qq("peioro malvi ") )
concordance <- add_row (concordance, malus)
concordance

bonus <- get_declension("bonus","good")
bonus
```

```{r}
bonus <- bonus %>% filter(! LatinStem %in% qq("melioro melium") )
concordance <- add_row (concordance, bonus)
concordance

```
```{r}
diligens <- get_declension("diligens","diligent")
diligens
```

```{r}
diligens <- diligens %>% filter (! LatinStem %in% qq("diligo"))
concordance <- add_row(concordance,diligens)
concordance
```
We run into another issue in the irregulat declension of multus where certain forms don't exist and are represented by  - 

```{r}
declension <- read_file("Data/multus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```
Added a rule to our regular expression 

```{r}
print(string_declension("multus"))
multus <- get_declension("multus","many")
multus 
```
And proceeded as usual 
```{r}
multus <- multus %>% filter (! LatinStem %in% qq("pluvi"))
concordance <- add_row(concordance,multus)
concordance
```

```{r}
solus <- get_declension("solus","only")
solus
```

```{r}
solus <- solus %>% filter (! LatinStem %in% qq("solvi soleo"))
concordance <- add_row(concordance,solus)
concordance
```

```{r}

declension <- read_file("Data/nullus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension
```

Another twist. this time it will take manual intervention to clean it.
First we will collect all the words. remove the extraneous endings then add the missing forms and then stem the result into a mini-concordance, and verify for valid forms.  We will modify get_declension to accept verba as a list.

Regular expressions were not matching characters as expected, using other methods to remove "ī" and 
"ō" from the list.  Got position by matching REGEX wildcard for single character entries  and then used lapply/sapply and indexing to remove the entries. 


```{r}
declension <- qq(string_declension("nullus"))
declension
# remove  single character entries 
remove <- lapply (declension, function(ch) grep ("^.$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 )]
declension
```

Next remove ae and then add the alternate entries
```{r}
remove <- lapply (declension, function(ch) grep ("^ae$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 ) ]
declension <- c(declension, qq("nūllī nūllae nūllī nūllō nūllae nūllō"))
declension


```
```{r}
nullus <- get_declension("nullus","none",forms = declension)
nullus
concordance<-add_row(concordance,nullus)
concordance
```
```{r}
unus <- get_declension("unus","one")
unus
unus <- unus %>% filter(!LatinStem %in% qq("unii unio"))
concordance <- add_row(concordance,unus)
concordance
```
```{r}
celer <- get_declension("celer","swift")
celer

celer <- celer %>% filter(!LatinStem %in% qq("celo celero"))
concordance <- add_row(concordance,celer)
concordance
```


```{r}
fortis <- get_declension("fortis","strong")
fortis
concordance <- add_row(concordance,fortis)
concordance
```
```{r}
vetus <- get_declension("vetus","old")
vetus
```
```{r}
vetus <- vetus %>% filter(!LatinStem %in% qq("i ii vetero vetvi vetero veto veterrimus veterrimvs vetustus"))
vetus
concordance<-add_row(concordance,vetus)
concordance


```
Adding a lookup function: 
```{r}
lookup = function(word) { 
  concordance %>% 
    filter(LatinStem == word) %>% 
    select(English)
}

```



This completes our proposed concordance.  Next we'll load in an original Latin manuscript, tokenize it , stem these words , Join our concordance to this then perform an frequency analysis on the result.   This will be our base line to compare translations. 

Using gutenbergr I will load the text of the Aenied in Latin and several of its english translations 
.

```{r}
gutenberg_metadata %>% select(title,author,gutenberg_id,language) %>% filter(grepl("[Ae|Æ]neid",title) & author=="Virgil")

TheLatinAeneid <- gutenberg_download(227)   
TheLatinAeneid
TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid228
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid18466
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid22456
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid29358
TranslationAeneid49844   <- gutenberg_download(49844)
TranslationAeneid49844


```
```{r}

TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid49844   <- gutenberg_download(49844)


```
Now that we have a source with concordance to look at and five different translations we can start to ask some questions.  First how do they compare in raw size? 

```{r}
Text_sizes <- tibble (title = "TheLatinAeneid", wordcount =  TheLatinAeneid %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid228", wordcount =  TranslationAeneid228 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid18466", wordcount =  TranslationAeneid18466 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid22456", wordcount =  TranslationAeneid22456 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid29358", wordcount =  TranslationAeneid29358 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid49844", wordcount =  TranslationAeneid49844 %>% unnest_tokens(word,text)%>% nrow())

Text_sizes
Text_sizes %>% ggplot( aes(title,wordcount))+
  geom_col(fill = ifelse(Text_sizes$title =="TheLatinAeneid", 'red','black'))+coord_flip()+theme(axis.text.x = element_text(angle = 90))
```


```{r}
theme(axis.text.x = element_text(angle = 90))
```


```{r}
+ coord_flip() +
  labs(title="Text wordcounts in various editions of 'TheAeneid' by Virgil")
```


Right away we can see that the information contained in each edition varies by a significant amount. This can be explained by front matter difference, different approaches to translation, and additional matter inserted into the edition.

So the next thing to attempt... can we see how faithful each edition is to the original? To do this we will attempt to construct a word frequency 'yardstick'   from the original Latin work and apply it to each of the editions in turn.  

```{r}
Analysis <-TheLatinAeneid %>% unnest_tokens(word,text)
Analysis$LatinStem<- Analysis$word %>% hunspell_stem( dict=latin ) %>% lapply( function(x) paste(x, collapse = " ")) %>% unlist()



#  con <- dbConnect(RSQLite::SQLite(), ":memory:")
# # 
#  dbCreateTable(con,"concordance",concordance)
#  dbWriteTable(con,"concordance",concordance,overwrite=T)
#  dbCreateTable(con,"analysis", Analysis)
#  dbWriteTable(con,"analysis", Analysis,overwrite=T)
# # 
# # 
#  dbListTables(con)
# # 
# #dbDisconnect(con)

 
# test <- dbGetQuery(con, "select word,analysis.LatinStem as LatinStems,concordance.LatinStem ,English from analysis,concordance where analysis.LatinStem like '% '||concordance.LatinStem or analysis.LatinStem like '% '||concordance.LatinStem||' %' or analysis.LatinStem like concordance.LatinStem||' %'") 
 
```
```{r}

Analysis<-Analysis %>% mutate(English="")
#foreach row of analysis 
for( row in 1:nrow(Analysis)){
  stem <- Analysis[row, "LatinStem"]
  if ( ! is_tibble(stem)){ 
    print("not a tibble")
  }else{
    #print(glue("row:{row}:tibble:{stem}"))
    #glimpse(stem)
    stems = qq(stem %>% pull())
    if (length(stems) < 1) {
      #print ("skipping")
      next
    }
    for(LatinStem in stems) {
      english <- lookup(LatinStem) %>% pull()
      if ( length(english ) == 0 ){
        next
      }else{
        Analysis[row,"English"] = english
      }
    }#end for 
  }#Else-fi
}#For row 
    


```


Now that we have an English word assigned to each Latin word that is in our concordance we can perform a word-frequency analysis on the original Latin work. 
```{r}

wf = Analysis %>% select(English) %>% filter(! English =="" ) %>% count(English)
wf



```
```{r}
concordance_words = as.vector( concordance %>% select(English) %>% pull())
wfAnalysis<- wf %>% pivot_wider(names_from = English, values_from = n) %>% mutate(WorkTitle = 'TheLatinAeneid') %>% select(WorkTitle,everything())
wfAnalysis



```
```{r}
# TranslationAeneid228     <- gutenberg_download(228)
# TranslationAeneid18466   <- gutenberg_download(18466)
# TranslationAeneid22456   <- gutenberg_download(22456)
# TranslationAeneid29358   <- gutenberg_download(29358)
# TranslationAeneid49844   <- gutenberg_download(49844)

wf = TranslationAeneid228 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid228') %>% select(WorkTitle,everything())

wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid18466 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid18466') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid22456 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid22456') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid29358 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid29358') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid49844 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid49844') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)


wfAnalysis
```
```{r}

ggplot(longwfAnalysis %>% filter(WorkTitle=="TheLatinAeneid" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis %>% filter(WorkTitle=="TranslationAeneid228" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis %>% filter(WorkTitle=="TranslationAeneid18466" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis %>% filter(WorkTitle=="TranslationAeneid22456" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis %>% filter(WorkTitle=="TranslationAeneid29358" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis %>% filter(WorkTitle=="TranslationAeneid49844" ), aes(words,count,fill=WorkTitle)) + geom_col()+theme(axis.text.x = element_text(angle = 90))

ggplot(longwfAnalysis, aes(words,count,fill=WorkTitle)) + geom_col(position = 'fill')+theme(axis.text.x = element_text(angle = 90))
```
## Conclusion

It appears that a collection of common adjectives can be shown to give a measure of 'reliability'  of a translation.  

## Further Research 

A number of things could enhance this investigation, expanding to multiple ediotions of the same book in the same language should show us a 'control'  in natural divergence as more material is added or revised in each edition.   

Additional context clues using NLP could enhance or eliminate a need for a concordance.  A broader concordance might capture a more solid signature. Development of a metric to quantify 'distance'  from the original or first edition could be ddeveloped based on the wf-signature of the original document.   



