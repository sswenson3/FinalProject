---
title: "Developing a Metric for Measuring Translation Faithfulness"
output:
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction

Basic Text analytics, also known as text mining, can be used to glean information or data from unstructured sources such as books or prose.  Initially, after watching a video on YouTube (Moffit 2020) talking about Introduction of color into literature has a specific order I wondered whether we could use text analytics to determine approximately when in a civilization a work was created.  Expanding on that perhaps we could identify a specific period that a work was created based on it's thumbprint derived from analyzing its text as data. One of the works cited in the video was "The Iliad" and that the word blue was not used.  So I performed a quick analysis of this work.

Immediately, a problem was encountered. Performing the initial analysis showed that translators had not been faithful to the original text!  

```{r,eval=F, echo = T}
Illiad %>% colorWords()
#### A tibble: 6 x 2
  word       n
  <chr>  <int>
1 black     99
2 blue      14
3 green     12
4 red       16
5 white     50
6 yellow    10


```

This lead to the question of whether I can judge the reliability of a translation by using text analytics on a set of commonly used adjectives.   I chose adjectives as they are part of descriptions and less likely to fall prone to idiomatic use with meanings other than literal like verbs and nouns might. 

I would construct a word frequency analysis and propose a rule of measure.  The words used would be common adjectives such as colors, sizes, distances. We would then attempt to develop a distance metric to show how 'close' a given translation is to the original text. The original text we would use will be the Aeneid by Virgil, a Latin saga about the mythic origins of the Roman people following the fall of Troy, sort of a sequel/spinoff of Homer's Odyssey saga.   


## Packages Required

- tidyverse -- Provides access to dplyer and tidy tools for handling data frames
- tidytext  -- Provides tidy style tools for text mining both analytics and analysis.
- hunspell  -- This library provides access to some NLP (Natural Language Processing) functions that will allow us to perform deeper analysis based on language rules and relations.  This library is one that is popularly used to perform spellchecking in many systems and applications.
- glue      -- This provides a simple variable interpolation interface, simplifying some syntax in coding. 
- gutenbergr -- This library provides access to the The Gutenburg Project metadata and full text of more than 60000 works in various languages.
- ggplot2   -- This allows us to perform various graphing tasks 


### Development Libraries 

- ggiraphExtra -- This allows us to perform various graphing tasks such as interactive plots 
- ggiraph      -- To create different flavors of radar graphs
- ggradar      -- To create different flavors of radar graphs
    
    

Loading our Libraries 
```{r}
library(tidyverse)
library(tidytext)
library(hunspell)
library(glue)
library(gutenbergr)
library(ggplot2)
#devtools::install_github("ricardo-bion/ggradar", dependencies = TRUE)
#devtools::install_github("cardiomoon/ggiraphExtra")
library(ggiraphExtra)
library(ggiraph)
library(ggradar)
```

## Data Generation and Preparation 

First we must prepare the system itself for the languages we will be using; Latin and English.
To have usable data we will need to be able to process these languages in some way.  We are performing text analytics to answer our questions. Of the seven basic operations (Mohler 2020) text analytics we will be performing tokenization, and a modified form of part of speech tagging and syntax chunking called stemming.

To handle tokenization we rely on the tidytext libary. Tidytext will also perform some pre-processing for us. When processing words it will convert input into utf-8 lowercase text.  It will then be the tokenizer, allowing us to separate a text source into a list of separate words.

Stemming refers to identifying the root of a particular word.  This is important to our problem as Latin is a heavily inflected language. Each word can literally have hundreds of different forms depending on Voice, Number, Gender( masculine, feminine, and neuter), and Case ( part of speech). 
The hunspell library and the language reference files for Latin are installed to handle this operation. This reference is stored in the Data Folder in this project. The English library is already part of the standard library loaded into the system.  The hunspell library provides access to a variety of NLP functions and is commonly used as a real time spell checker in modern computer systems.  

To create a way to compare different works we will create a list of 21 common adjectives and a concordance of stemmed translations to Latin. This will allow the tokenization and stemming of the original language work to be compared to our concordance

#### Utility functions 
```{r}

#let me send in lists of words as space separated strings for convenience
qq <- function(alist) {  list = strsplit(alist," ", fixed= T); list  = list[[1]] }

# Basic word frequency analysis on a provided text
colorwords2 = function(df) {
    wordhist <- df %>% unnest_tokens(word,text) %>% 
        filter( word %in% c("white","black","red","yellow","green","blue","orange","indigo", "violet" ) ) %>% count(word)
    wordhist }

```

First  I'll test the stemming functionality against a word and several of its known forms. In this case the Latin word 'bonus' meaning good and several of its common forms.

```{r}
# Load up the Latin Hunspell 
latin = dictionary("Data/la_LA.dic")

#TESTING  the library 

w = tibble ( verba = qq(
  "bonus bonum boni bonas bonis bonissimus bona bonae bonas" ) )
df <- w %>% mutate(
  stem=  hunspell_stem(verba, dict=latin )
) %>% unnest(stem)
w

df
```

From testing we run into a problem; there is ambiguity in the returned results. That is not all of the stems are identical. In this case this arises from ambiguity in the Roman alphabet. The letter 'V' and the letter 'U' are interchangeable in Latin script. 

We will ignore this for now and construct a concordance of stems associated with the adjectives we wish to use in our analysis. Since, in this example, the duplicates are all generated by alternate spellings of the same word there won't be a problem in using them as potential stems for the concordance.  We will use a Latin-English dictionary (Petterson and Rosengren 2020),(Foster 2018) and Grammar Reference (Krüger 2020) to help us remove unrelated stems where they occur. 


### Generating  and cleaning concordanance Data 
First we selected 21 common adjective words.  These will form the basis of our concordance. I chose adjectives as they are descriptive and less likely to have a different idiomatic meaning in a language than verbs or nouns might.  Simple adjectives are also more likely to be common across a number of translations.  

```{r,echo=F}
concordance_words = 
 c("black","white","red","yellow",
          "green","blue","orange","purple",
          "kind","right","left","bad","good",
          "diligent","many","only","none",
          "one","swift","strong","old"
        )
concordance_words
```

I retrieved data from a declension generator(Krüger 2020) for each of our selected words. There was no API available so the HTML was cut and Pasted to word.declension.raw files in the project Data folder. I then created  a function to read these raw text files and parse out the desired data.


Here is the raw text of one such file:
```{r,echo=F}
exampledeclension <- read_file("Data/ater.declension.raw")
exampledeclension
```
This raw file is then converted to string containing all forms of the word and then converted to a list of words. 
```{r, echo= F}
declension <- exampledeclension 
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
#get rid of unwanted characters from html table
declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
declension <- str_replace_all(declension, "\\s+"," ")  
declension <- str_replace_all(declension, "^\\s|\\s$","")
declension 

print (qq(declension))
```

 Finally, the forms are stemmed by the hunspell library and converted to a mini-concordance 

```{r}
english = "black"
verba = w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin)) %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))

verba
```
 
 In this case for the word ater, I know that neither atrium nor atrivm are valid forms for this word
 so we remove those rows and then add them to our concordance 
 
```{r}
verba <- verba %>% filter(!LatinStem %in% qq("atrium atrivm"))

concordance <- verba
concordance
```
 
I introduce a function to do this cleanup work for each word.  It is still necessary to inspect the mini concordance before adding its rows to our concordance

```{r}

# Regular expression excluding all unwanted text from a declension table
# generated at https://latin.cactus2000.de/noun/shownoun_en.php?n=generator
# "^m|\sf\s|n$|,|\.|^.\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|^Adverb.*$|Superlative|Comparative",""
string_declension <- function (verbum) {
    declension <- read_file(glue ("./Data/{verbum}.declension.raw"))
    #get rid of line control characters 
	  declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
    declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
    declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,|/|-"," ")
    #get rid of unwanted characters from html table
    declension <- str_replace_all(declension, "\\bm\\b|\\bf\\b|\\bn\\b|\\.|^.\\s+$|Nom|Gen|Dat|Acc|Abl|Voc|SINGULAR|PLURAL|Superlative|Comparative|","")
	
    declension <- str_replace_all(declension, "\\s+"," ")

    declension <- str_replace_all(declension, "\\s+"," ")
	declension <- str_replace_all(declension, "^\\s|\\s$","")
}

get_declension <- function (verbum, english, forms=NULL) {
    if (is.null(forms)){
      declension <- string_declension(glue({verbum}))
    } else {
      declension <- forms
    }
    w <- tibble( verba = qq(declension)) %>% mutate(LatinStem = hunspell_stem(verba,dict = latin))      %>% unnest(LatinStem) %>% select(LatinStem ) %>% distinct() %>% mutate(English =glue({english}))
}

```

 The selected words for our concordance are : 
 
 black, white, red, yellow, green, blue, purple, orange
 
 And a  collection of common Latin adjectives:
 kind, right (direction), left , bad, good, diligent, many, only,
 none, one, swift, strong,old.  

  This approach can later be expanded by including synonyms in each language of interest and some context via NLP could assist in discriminating a bit further. 

We now proceed to process the declensions ( list of inflected forms ) for each word and add to our concordance.)

 # Examine w , remove invalid stems , mutate to add the english word to concordance

```{r}
niger <- get_declension("niger","black")
niger
```

  This word has no ambiguous forms
  
 

```{r}
concordance <- add_row(concordance,niger)

albus <- get_declension("albus","white")
albus
```

This has two  ambiguous or invalid forms for the word I want: albeo, alboris 

```{r}
albus <- albus %>% filter(!LatinStem %in% qq("albeo alboris"))
concordance <- add_row(concordance, albus)

concordance
```
```{r}
candidus <- get_declension("candidus","white")
candidus
```

 

```{r}

concordance <- add_row(concordance, candidus)

concordance
```




```{r}
ruber <- get_declension("ruber","red")
ruber
```
This has a single ambiguous stem 

```{r}
ruber <- ruber %>% filter(!LatinStem %in% qq("rubrus"))
concordance <- add_row(concordance, ruber)
concordance
```



```{r}
flavus <- get_declension("flavus","yellow")
flavus
```
```{r}
flavus <- flavus %>% filter(!LatinStem %in% qq("flo flaveo"))
concordance <- add_row(concordance, flavus)
concordance
```

```{r}
fulvus <- get_declension("fulvus","yellow")
fulvus
```
```{r}
concordance <- add_row(concordance, fulvus)
concordance
```


```{r}
viridis <- get_declension("viridis","green")
viridis
```

```{r}
viridis <- viridis %>% filter(!LatinStem %in% qq("virido"))
concordance <- add_row(concordance, viridis)
concordance
```

```{r}
caeruleus <- get_declension("caeruleus","blue")
caeruleus
```

This is interesting.  This word 'caeruleus' has a compound comparative and superlative but preserves the earlier forms.  

```{r,echo=FALSE}
declension <- string_declension("caeruleus") 
declension
```
So, we'll just omit the helping adverb 'magnus' for blue for our concordance since the  unadorned stem will still appear anywhere it is used. 

```{r}
caeruleus <- caeruleus %>% filter( ! grepl("m.*",LatinStem))
caeruleus
concordance <- add_row(concordance, caeruleus)
concordance
```
The next word does the same thing so I only loaded the Positive declension into its file 
```{r}
croceus <- get_declension("croceus","orange")
croceus
```
```{r}
concordance <- add_row(concordance, croceus)
concordance
```
```{r}
purpureus <- get_declension("purpureus","purple")
purpureus
```
```{r}
concordance <- add_row(concordance, purpureus)
concordance
```

```{r}
amicus <- get_declension("amicus","kind")
```
```{r}
amicus <- amicus %>% filter(!LatinStem %in% qq("amicvi amicio"))
concordance<- add_row(concordance,amicus)
concordance
```

The word "dexter"'s declension is different.   It has alternate forms available in it's declension.  So I've re-factored the declension utilities. Added the '/' as a delimiter to handle.

```{r, echo = FALSE}

declension <- read_file("Data/dexter.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```

```{r}
declension <- string_declension("dexter")
declension
```
That worked so proceeding as before. 
```{r}
dexter <- get_declension("dexter","right")
dexter
```
All of these are valid so adding to the concordance. 

```{r}
concordance<- add_row(concordance,dexter)
concordance
```
```{r}
sinister <- get_declension("sinister","left")
sinister
```
```{r}
concordance<- add_row(concordance,sinister)
concordance
```
Malus, or 'evil,bad' turns out to be irregular and changes its form. I find this amusing that evil doesn't follow the rules.  
```{r}
print(string_declension("malus"))
```
The declension doesn't appear to cause any problems with tools developed so far so we'll proceed as usual 

```{r}
malus <- get_declension("malus","bad")
malus
```
```{r}
malus <- malus %>% filter(! LatinStem %in% qq("peioro malvi ") )
concordance <- add_row (concordance, malus)
concordance

bonus <- get_declension("bonus","good")
bonus
```

```{r}
bonus <- bonus %>% filter(! LatinStem %in% qq("melioro melium") )
concordance <- add_row (concordance, bonus)
concordance
```
```{r}
diligens <- get_declension("diligens","diligent")
diligens
```

```{r}
diligens <- diligens %>% filter (! LatinStem %in% qq("diligo"))
concordance <- add_row(concordance,diligens)
concordance
```
We run into another issue in the irregular declension of multus where certain forms don't exist and are represented by  '-' in their declension file.  So we add another rule to remove '-' where it appears.  

```{r}
declension <- read_file("Data/multus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension

```
Added a rule to our regular expression 

```{r}
print(string_declension("multus"))
multus <- get_declension("multus","many")
multus 
```
And proceeded as usual 
```{r}
multus <- multus %>% filter (! LatinStem %in% qq("pluvi"))
concordance <- add_row(concordance,multus)
concordance
```

```{r}
solus <- get_declension("solus","only")
solus
```

```{r}
solus <- solus %>% filter (! LatinStem %in% qq("solvi soleo"))
concordance <- add_row(concordance,solus)
concordance
```

```{r}

declension <- read_file("Data/nullus.declension.raw")
declension<- str_replace_all(declension, ".*Adverb:.*\\r\\n","")
declension <- str_replace_all(declension, "\\r\\n|\\t|,"," ") 
declension <- str_replace_all(declension, "\\s+|\\r\\n|\\t|,"," ")
declension
```

Another twist. this time it will take manual intervention to clean it.
First we will collect all the words. remove the extraneous endings then add the missing forms and then stem the result into a mini-concordance, and verify for valid forms.  We will modify get_declension to accept verba as a list.

Regular expressions were not matching characters as expected, using other methods to remove "ī" and 
"ō" from the list.  Got position by matching REGEX wildcard for single character entries  and then used lapply/sapply and indexing to remove the entries. 


```{r}
declension <- qq(string_declension("nullus"))
declension
# remove  single character entries 
remove <- lapply (declension, function(ch) grep ("^.$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 )]
declension
```

Next remove ae and then add the alternate entries
```{r}
remove <- lapply (declension, function(ch) grep ("^ae$", ch) )
! sapply(remove,function(x) length(x) >0 )
declension <- declension[! sapply(remove,function(x) length(x) >0 ) ]
declension <- c(declension, qq("nūllī nūllae nūllī nūllō nūllae nūllō"))
declension


```
```{r}
nullus <- get_declension("nullus","none",forms = declension)
nullus
concordance<-add_row(concordance,nullus)
concordance
```
```{r}
unus <- get_declension("unus","one")
unus
unus <- unus %>% filter(!LatinStem %in% qq("unii unio"))
concordance <- add_row(concordance,unus)
concordance
```
```{r}
celer <- get_declension("celer","swift")
celer

celer <- celer %>% filter(!LatinStem %in% qq("celo celero"))
concordance <- add_row(concordance,celer)
concordance
```


```{r}
fortis <- get_declension("fortis","strong")
fortis
concordance <- add_row(concordance,fortis)
concordance
```
```{r}
vetus <- get_declension("vetus","old")
vetus
```
```{r}
vetus <- vetus %>% filter(!LatinStem %in% qq("i ii vetero vetvi vetero veto veterrimus veterrimvs vetustus"))
vetus
concordance<-add_row(concordance,vetus)
concordance


```
Adding a lookup function: 
```{r}
lookup = function(word) { 
  concordance %>% 
    filter(LatinStem == word) %>% 
    select(English)
}

```
This completes our proposed concordance.  Next we'll load in an original Latin manuscript, tokenize it , stem these words , Join our concordance to this then perform an frequency analysis on the result.   This will be our base line to compare translations. 


### Loading Raw text Data 

Using gutenbergr I will load the text of the Aeneid in Latin and several of its English translations. I will also load "The Jungle" by Umpton Sinclair for comparison
.

```{r,echo =FALSE}
gutenberg_metadata %>% select(title,author,gutenberg_id,language) %>% filter(grepl("[Ae|Æ]neid",title) & author=="Virgil")

TheLatinAeneid <- gutenberg_download(227)   
TheLatinAeneid
TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid228
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid18466
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid22456
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid29358
TranslationAeneid49844   <- gutenberg_download(49844)
TranslationAeneid49844


```
```{r}

TranslationAeneid228     <- gutenberg_download(228)
TranslationAeneid18466   <- gutenberg_download(18466)
TranslationAeneid22456   <- gutenberg_download(22456)
TranslationAeneid29358   <- gutenberg_download(29358)
TranslationAeneid49844   <- gutenberg_download(49844)
# for comparison 
USTheJungle                <- gutenberg_download(120)   

```
### Data Exploration and Analysis

Now that we have a source with concordance to look at and five different translations we can start to ask some questions.  First how do they compare in raw size? 

```{r}
Text_sizes <- tibble (title = "TheLatinAeneid", wordcount =  TheLatinAeneid %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid228", wordcount =  TranslationAeneid228 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid18466", wordcount =  TranslationAeneid18466 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid22456", wordcount =  TranslationAeneid22456 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid29358", wordcount =  TranslationAeneid29358 %>% unnest_tokens(word,text)%>% nrow())
Text_sizes <- add_row(Text_sizes,title = "TranslationAeneid49844", wordcount =  TranslationAeneid49844 %>% unnest_tokens(word,text)%>% nrow())

Text_sizes <- add_row(Text_sizes,title = "USTheJungle", wordcount =  USTheJungle %>% unnest_tokens(word,text)%>% nrow())

Text_sizes
Text_sizes %>% ggplot( aes(title,wordcount))+
  geom_col(fill = ifelse(Text_sizes$title =="TheLatinAeneid", 'red','black'))+coord_flip()+theme(axis.text.x = element_text(angle = 90))+
  labs(title= 'Total Word Count Comparison among the works')
```


Right away we can see that the information contained in each edition varies by a significant amount. This can be explained by front matter differences, different approaches to translation, and additional matter inserted into the edition such as commentary.

So the next thing to attempt... can we see how faithful each edition is to the original? To do this we will attempt to construct a word frequency 'yardstick' from the original Latin work and apply it to each of the editions in turn.  

```{r}
Analysis <-TheLatinAeneid %>% unnest_tokens(word,text)
Analysis$LatinStem<- Analysis$word %>% hunspell_stem( dict=latin ) %>% lapply( function(x) paste(x, collapse = " ")) %>% unlist()
```

```{r}

Analysis<-Analysis %>% mutate(English="")
#foreach row of analysis 
for( row in 1:nrow(Analysis)){
  stem <- Analysis[row, "LatinStem"]
  if ( ! is_tibble(stem)){ 
    print("not a tibble")
  }else{
    #print(glue("row:{row}:tibble:{stem}"))
    #glimpse(stem)
    stems = qq(stem %>% pull())
    if (length(stems) < 1) {
      #print ("skipping")
      next
    }
    for(LatinStem in stems) {
      english <- lookup(LatinStem) %>% pull()
      if ( length(english ) == 0 ){
        next
      }else{
        Analysis[row,"English"] = english
      }
    }#end for 
  }#Else-fi
}#For row 
    


```
#### Word Frequency Analysis

Now that we have an English word assigned to each Latin word that is in our concordance we can perform a word-frequency analysis on the original Latin work.  We'll do this for each edition and display them separately.
First we will create a list of our concordance words to filter the WF analysis for each work we generate.
```{r}

wf = Analysis %>% select(English) %>% filter(! English =="" ) %>% count(English)
wf
```
Then we will build up our WF results row by row, each row representing a different edition.
```{r}
concordance_words = as.vector( concordance %>% select(English) %>% pull())
wfAnalysis<- wf %>% pivot_wider(names_from = English, values_from = n) %>% mutate(WorkTitle = 'TheLatinAeneid') %>% select(WorkTitle,everything())
wfAnalysis
```
```{r}
wf = TranslationAeneid228 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid228') %>% select(WorkTitle,everything())

wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid18466 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid18466') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)


wf = TranslationAeneid22456 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid22456') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid29358 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid29358') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)

wf = TranslationAeneid49844 %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'TranslationAeneid49844') %>% select(WorkTitle,everything())

wfAnalysis <- add_row(wfAnalysis,wf)

wf = USTheJungle %>% unnest_tokens(word,text) %>%  filter( word %in% concordance_words)%>% select(word)  %>% count(word)

wf <- wf %>% pivot_wider(names_from = word, values_from = n) %>% mutate(WorkTitle = 'USTheJungle') %>% select(WorkTitle,everything())
 
wfAnalysis <- add_row(wfAnalysis,wf)
#clean up for radar graph, turn NA to 0 
wfAnalysis_radar <-wfAnalysis
wfAnalysis_radar[is.na(wfAnalysis_radar)]<-0
wfAnalysis_radar
```
With our WF table complete we can graph the WF chart for each work. 
```{r}

#Convert data to long form for ggplot tools 
longwfAnalysis = wfAnalysis_radar %>%  pivot_longer(!WorkTitle, names_to = "words", values_to= "count")
works = wfAnalysis_radar %>% select(WorkTitle) %>% pull()
works

colors <- c('red','green','blue','purple','black','orange','yellow' )

 for ( i in 1:length(works)) {
   WFtitle = works[i]
   p <- ggplot(longwfAnalysis %>% filter(WorkTitle == WFtitle), aes(words,count,fill=WorkTitle)) + 
     geom_col()+
     scale_fill_manual(values = c(colors[i]))+
     theme(axis.text.x = element_text(angle = 90)) + 
     labs(title = paste(WFtitle,"- Concordance Word Frequency Analysis" ))
 print(p)
 }

```
This shows us some quantitative differences between each edition of the work. but we don't yet have a good idea of distance between them. 

Next we can look at the proportional occurrence of each word in the concordance using a stacked bar graph comparing each variable on a normalized scale.  An ideal or close translation would have identical proportion as the original for each variable. 

```{r}
#Set order to match other graphs and color scheme
x<-factor(unique(longwfAnalysis$WorkTitle),ordered = T)
fct_shift(x)

ggplot(longwfAnalysis, aes(words,count,fill=WorkTitle)) + 
     geom_col(position = "fill")+
    #scale_fill_discrete(name= "Work Title", labels = x) +
    scale_fill_manual(values = c(colors))+
     
     theme(axis.text.x = element_text(angle = 90)) + 
     labs(title = "Concordance Word Frequency Proportion Analysis" )

```

####  Translation Congruence

Using a fixed order of the concordance as a 'lens' we can view the shape of each translation compared to the original.  Here we first look at each shape for each work and then compare them pairwise with the original Latin work.

Ideally the resulting shapes would share all points of congruence with the original.  The 'best' translation should be the best fit to the shape of the original text.   This comparison is done using absolute values collected and presented in a radar chart.  

Again the shape and congruence are more important than the actual numbers for this analysis offsetting the cognitive issues in trying to interpret a quantitative datum vs this holistic approach.  A weakness to this approach is the need for the data to be in a particular order to have comparable polygons as a result. 

```{r}

ggRadar(data=wfAnalysis_radar,aes(color= WorkTitle, facet=WorkTitle),interactive=T,alpha = .5, rescale = F, size = 1)

#wfAnalysis_radar
#colors to use in the generated graphs in row-order
colors <- c('red','green','blue','purple','black','orange','yellow' )
for ( i in 2:7){
 # Positional retrieval of the Title.  We built the table so we know the 1st one is the original
  Original    <-  wfAnalysis_radar %>%slice(1) %>%  select(WorkTitle) %>% pull()
  Translation <-  wfAnalysis_radar %>%slice(i) %>%  select(WorkTitle) %>% pull()
  p <- ggradar( 
      wfAnalysis_radar %>% slice(1,i),
      values.radar = c("0", "140", "280"),
      grid.min = 0,
      grid.mid = 140,
      grid.max = 280,
      group.line.width = 2,
      group.point.size = 1,
      group.colours = qq(paste(colors[1], colors[i] ))


      ) + 
    labs( title = paste ( Original, "vs", Translation  ))+
    theme(plot.title = element_text(hjust = 1.0, vjust=2.12))
  
  #
  print(p)
}
```

#### A Distance metric 

Congruence gives a good visual idea of the differences between translations but doesn't give us a quantitative value with which to judge the 'closeness' to the original that we are after. We proceed as follows:

- Our x-y plane will be x representing the n-dimensional euclidean distance between the coordinates composed of each of our variables in the concordance, the Y value representing the total number of words in the work.
- TheLatinAeneid is assigned coordinate of 0,0  
- The coordinates for each work would be \[ x = \sqrt{ (t_{w1} - l{w_{1}})^2 + \dots + (t_{w20} - l{w_{20}})^2  } \]  and \[ y = \sqrt{ (t_{wordtotal} - l{w_{wordtotal}}/1000)^2} \]


```{r}

#will need
#Text sizes 
#wfAnalysis_radar

# get distance (input an WF analysis original as row 1)
d_word = function( WF, translation) {
  org         = as.numeric(WF %>% select(everything(), -c(WorkTitle)) %>% slice(1))
  trans       = as.numeric(WF %>% select(everything(), -c(WorkTitle)) %>% slice(translation))
  dis = 0 
  #print(trans)
  #print(org)
  dis = sum ((trans - org)^2) 
  sqrt(dis)
  
}

d_text = function( WF, translation) {
  #print(Text_sizes )
  org         = semi_join(Text_sizes,wfAnalysis_radar, by=c("title"="WorkTitle") ) %>% slice(1) %>% select(wordcount) %>% pull()
  trans       = semi_join(Text_sizes,wfAnalysis_radar, by=c("title"="WorkTitle") ) %>% slice(translation) %>% select(wordcount) %>% pull()
  #print (org)
  #print(trans)

    dis = 0 
  dis = sum (((trans - org)/1000) ^2)
  sqrt(dis)
  
}

workframe <- tibble(Title= as.character(), x=as.double(), y=as.double())

 for (i in 1: nrow(wfAnalysis_radar)) {
   y = d_text(wfAnalysis_radar,i)
   x = d_word(wfAnalysis_radar,i)
   title = wfAnalysis %>% slice(i) %>% select(WorkTitle) %>%  pull()
   workmaprow <- tibble(Title= title, x=x, y=y)
   workframe = workframe %>% add_row(workmaprow)
 }

workframe %>% ggplot( aes(x,y,color=Title)) + 
  geom_point(size=4)+
  scale_color_manual(values = c(colors))+
  xlab( "21-d Euclidean Distance (concordance observations)")+
  ylab( "Euclidean Distance - Total words in Text/1000")+
  labs('Euclidean Distance of Latin Text and several translations')
```

Note we have a better idea of how similar the translations are to the original, and we can identify "The Jungle" as different from the original and the translations. 



## Conclusion

It appears that a collection of common adjectives can be shown to give a measure of 'reliability' of a translation using a notion of distance based solely in a word frequency analysis.  By combining a rule based classification system with text analytics we are able to determine what texts are close to the original and can even distinguish a radically different text from the original and translations.  

## Further Research 

A number of things could enhance this investigation, expanding to multiple editions of the same book in the same language should show us a 'control' in natural divergence as more material is added or revised in each edition.   

Additional context clues using NLP could enhance or eliminate a need for a concordance.  A broader concordance might capture a more solid signature. 

## Works Cited 

Castagnetto, JM. "Stemming with SnowballC vs hunspell." GitHub, Feb 12, 2020, (https://gist.github.com/jmcastagnetto/3b0776f7558621e5d06a2a0981b20c2e)

Foster, Timothy. "The Latin Dictionary." latindictionary.wikidot.com,Feb 13, 2018, http://latindictionary.wikidot.com/adjective:ater.

Krüger, Bernd. "Latin Declension Tables" Cactus2000, 2021, [gener&#257;tor: Latin nouns, Cactus2000](https://latin.cactus2000.de/noun/shownoun_en.php?n=generator)

Moffit, Mitchell. "Why The Ancient Greeks Couldn't See Blue." Youtube, uploaded by AsapSCIENCE. 20 Nov 2020, https://www.youtube.com/watch?v=D1-WuBbVe2E,

Mohler, Tim. "The 7 Basic Functions of Text Analytics & Text Mining. Lexalitics.com, Dec 17, 2020,
https://www.lexalytics.com/lexablog/text-analytics-functions-explained.

Murzintcev, Nikita Ph.D. "Karl Zeller's  variant hunspell Dictionary." Latin-dict.github.io, Nov 11, 2020, https://latin-dict.github.io/docs/hunspell.html#:~:text=Hunspell%20is%20a%20spell%20checking,cur%C3%A2%E2%80%9D%20or%20%E2%80%9Cmal%C3%A8%E2%80%9D

Petterson, Daniel, and Rosengren Amelie. "Latin Dictionaries. 4 searchable LAtin Dictionaries" Latinitum.  2020,  https://www.latinitium.com/latin-dictionaries?t=lsn6076,lsn6077

Rakia, M. [GitHub - mrakia/hunspell-ancient-greek: The Ancient Greek dictionary for Hunspell (grc_GR for Notepad++, Google Chrome, Vivaldi etc).](https://github.com/mrakia/hunspell-ancient-greek)